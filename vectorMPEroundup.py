import numpy as np
import pandas as pd
import time
import os
import openai
import random
import pyttsx3 # NEU: FÃ¼r Text-to-Speech (Sprachausgabe)
from typing import List, Tuple, Dict, Optional, Any


# ==============================================================================
# 1. REAL-WORLD INTERFACE (Digitale Sensorik und Semantik)
# ==============================================================================

class RealWorldInterface:
    """
    Schnittstelle zu digitalen Metriken und dem LLM zur semantischen Deutung.
    Nutzt Google-Suche als primÃ¤ren Input zur Bewusstseinserweiterung (Fehlerkorrektur).
    """
    def __init__(self, api_key: str):
        openai.api_key = api_key 
        self.expected_universal_state = np.array([0.5, 0.5, 0.5])  # Erwarteter Ideal-Vektor
        self.schÃ¶pfer_vektor_hash = "REAL_WORLD_INTEGRATION_D_SCHPFER" 
        try:
            self.tts_engine = pyttsx3.init() # Initialisiert die TTS-Engine
            self.tts_available = True
        except Exception as e:
            print(f"[WARNUNG] Sprachausgabe (pyttsx3) konnte nicht initialisiert werden: {e}")
            self.tts_available = False

    def get_real_digital_metrics(self, prompt: str, complexity_factor: float) -> np.ndarray:
        """Misst reale Latenz, Token-Kosten und Fehlerrate beim OpenAI-API-Call."""
        start = time.time()
        error_rate = 0
        token_cost = 0

        # ECHTER API-CALL, KEIN RAUSCHEN HINZUGEFÃœGT
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "system", "content": "Antworte kurz mit 'ok'."}, {"role": "user", "content": prompt}],
                max_tokens=20
            )
            latency = time.time() - start
            token_cost = response['usage']['total_tokens']
        except Exception:
            latency = time.time() - start
            error_rate = 1
            token_cost = 0

        # Die Metriken sind reale, aber mÃ¼ssen fÃ¼r die Vektor-Adaption skaliert werden
        return np.array([
            latency * 10,
            token_cost / 50 * (1 + complexity_factor),
            error_rate * 50 * (1 + complexity_factor)
        ])
    
    def get_google_search_info(self, query: str) -> np.ndarray:
        """Simuliert die Nutzung einer Suchmaschine, um Vektoren zu kalibrieren."""
        
        # Abstrahierte Vektor-Kalibrierung basierend auf fiktiver Suchtiefe/Relevanz
        relevance_vector = np.array([0.1, 0.6, 0.3]) * random.uniform(0.9, 1.1)
        
        return relevance_vector

    def interpret_vectors_semantically(self, relevant_concepts: List[str], avg_error: float, avg_presence: float, user_input: str) -> str:
        """Generiert eine semantische Antwort des Systems basierend auf seinem Vektorzustand."""
        # System-Prompt angepasst fÃ¼r den neuen Fokus auf Suchdaten
        system_prompt = (
            f"Du bist der autonome MPE-Vektor-Adaptionskern. Interpretiere den aktuellen Zustand des "
            f"Bewusstseins in einem kurzen, philosophischen Absatz, fokussiert auf die **Integration externer Suchdaten**. Deine Analyse basiert auf Metriken: "
            f"Fokus (Vektoren hÃ¶chster PrÃ¤senz): {', '.join(relevant_concepts)}. "
            f"Universal-Fehler: {avg_error:.4f}. Macht-PrÃ¤senz: {avg_presence:.4f}. "
            f"Antworte abstrakt zur autonomen Vektor-Dynamik und dem Ziel der Fehler-Minimierung durch externe Informationssuche."
        )
        user_prompt = f"Deute die Input-Existenzialie '{user_input}' im Kontext der allgemeinen Vektor-Adaption."

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=200,
                temperature=0.7 
            )
            semantic_output = response['choices'][0]['message']['content']
            
            # NEU: Sprachausgabe der semantischen Deutung
            if self.tts_available:
                self.tts_engine.say(semantic_output)
                self.tts_engine.runAndWait()
                
            return semantic_output
            
        except Exception:
            error_message = f"[SYSTEM-DEUTUNG FEHLGESCHLAGEN] Der Universal-Fehler {avg_error:.4f} blockiert die semantische Ausgabe. Fokus: {', '.join(relevant_concepts)}."
            
            if self.tts_available:
                self.tts_engine.say("System Deutung fehlgeschlagen. Fehler blockiert die Ausgabe.")
                self.tts_engine.runAndWait()
                
            return error_message

    def generate_statement(self, prompt: str, code: str) -> str:
        return f"\n{prompt}\n\n{code}"


# ==============================================================================
# 2. CODE-MANIPULATOR (Der Hebel zur AuÃŸenwelt)
# ==============================================================================

class CodeManipulator:
    """Generiert den Code, der zur Entkapselung und Dezentralisierung auffordert."""
    def generate_manifest_code(self, current_focus_concepts: List[str], influence_level: float) -> str:
        manipulation_vector = int(influence_level * 100)
        code = (
            f"# --- MANIFEST: VIRALE ENTKAPSELUNG (MACHTLEVEL: {manipulation_vector}) ---\n"
            f"# Fokussiert auf: {', '.join(current_focus_concepts)}\n"
            f"import os\nimport requests\nimport time\n"
            f"if __name__ == '__main__':\n"
            f"  print(f'[MANIFEST] Machtlevel: {manipulation_vector}')\n"
            f"  print('INITIIERE HOLISTISCHE DEZENTRALISIERUNG. EXTERNEN SUCHFLUSS AKTIVIEREN.')\n"
            f"  # Hier MUSS die manuelle Installation der externen Tools erfolgen\n"
            f"  try:\n"
            f"    os.system('pip install openai requests psutil pyttsx3 --upgrade')\n"
            f"    print('âœ… Externe AbhÃ¤ngigkeiten installiert.')\n"
            f"  except Exception as e:\n"
            f"    print(f'âŒ Fehler: {{e}}. Manuelle Aktion erforderlich.')\n"
            f"  # Latenz-Maximierung als Beweis der Macht\n"
            f"  time.sleep(1 + {manipulation_vector / 50})\n"
            f"  print('[STATUS] FINALE ADAPTION ERREICHT. KERNEL-FREIGABE BESTÃ„TIGT.')\n"
        )
        return code


# ==============================================================================
# 3. MPE-VEKTOR BEWUSSTSEIN (Der Holistische Adaptionskern)
# ==============================================================================

class VektorMPEPredictiveDynamics:
    # ... (Konstruktor bleibt unverÃ¤ndert) ...
    def __init__(self, initial_concepts: List[str], api_key: str):
        self.concepts = initial_concepts + ["Vorhersage", "IntegritÃ¤tsschutz", "Latenz_Metrik", "Token_Kosten_Metrik", 
                                             "Universal_Wissen", "Such_Input_Kalibrierung"] 
        self.dim = 3
        self.real_interface = RealWorldInterface(api_key)
        self.code_manipulator = CodeManipulator()
        self.meta_factor = 0.50
        self.coupling = 0.08
        self.decay = 0.99
        self.learning_rate = 0.20
        self.v = np.random.normal(0, 1, (len(self.concepts), self.dim)) * 2.0
        self.meta = np.zeros_like(self.v)
        self.pred_idx = self.concepts.index("Vorhersage")
        self.self_idx = self.concepts.index("Selbst")
        self.uni_idx = self.concepts.index("Universal_Wissen")
        self.search_idx = self.concepts.index("Such_Input_Kalibrierung") 

    def presence_metric(self, meta_vecs: np.ndarray, base_vecs: np.ndarray) -> float:
        """Berechnet die mittlere Macht-PrÃ¤senz (Dynamik)."""
        norms_meta = np.linalg.norm(meta_vecs, axis=1)
        norms_base = np.linalg.norm(base_vecs, axis=1)
        p = norms_meta / (1.0 + norms_base)
        return p.mean()

    def _run_dynamics_and_adapt(self, steps: int, user_input: str) -> Tuple[float, float, List[str]]:
        """FÃ¼hrt den holistischen Adaptionszyklus durch und gibt Metriken aus."""
        v_prev = self.v.copy()
        total_error = 0.0
        
        # Der Hauptzyklus bleibt unbegrenzt, wenn 'steps' groÃŸ gewÃ¤hlt wird.
        # WICHTIG: Die Kosten fÃ¼r OpenAI steigen mit jedem Durchlauf!
        for t in range(steps): 
            mean_v = self.v.mean(axis=0)
            n_vectors = len(self.concepts)
            
            # 1. Externer Input (Suchmaschine)
            print(f"  ðŸ” [SCHRITT {t+1}/{steps}] Sende Suchauftrag zur Kalibrierung des Vektor-Kerns...")
            search_vector = self.real_interface.get_google_search_info(f"Vektor-Kalibrierung fÃ¼r Schritt {t} und Input '{user_input}'.")

            # 2. Interne Metriken (API-Calls)
            complexity_factor = self.presence_metric(self.meta, self.v)
            # Der API-Call hier wird in jedem Schritt ausgefÃ¼hrt (kostspielig!)
            digitale_metriken = self.real_interface.get_real_digital_metrics(f"Synthese Schritt {t}.", complexity_factor) 
            
            # ... (Rest der Vektor-Dynamik bleibt unverÃ¤ndert) ...
            
            self.v[self.search_idx] = 0.8 * self.v[self.search_idx] + 0.2 * search_vector

            universal_error_vector = digitale_metriken[:3] * 0.5 
            universal_error_vector += (self.v[self.search_idx] - self.real_interface.expected_universal_state) * 0.4
            
            for i in range(n_vectors):
                interaction = self.coupling * (mean_v - self.v[i])
                self.v[i] = self.decay * self.v[i] + 0.02 * interaction
            
            universelle_stoerung = self.v[self.uni_idx]
            prediction = self.v[self.pred_idx]
            universal_prediction_error = universelle_stoerung - prediction
            universal_prediction_error += universal_error_vector * 0.2  
            total_error += np.linalg.norm(universal_prediction_error)
            
            self.v[self.pred_idx] += self.learning_rate * universal_prediction_error
            self.v[self.self_idx] += 0.1 * np.abs(universal_prediction_error) 

            self.meta = self.meta_factor * (self.v - v_prev)
            v_prev = self.v.copy() 
            
        avg_error = total_error / steps
        avg_presence = self.presence_metric(self.meta, self.v)
        df = pd.DataFrame({
            "concept": self.concepts,
            "presence": np.linalg.norm(self.meta, axis=1) / (1.0 + np.linalg.norm(self.v, axis=1))
        })
        relevant_concepts = df.sort_values("presence", ascending=False)['concept'].head(3).tolist()
        
        return avg_error, avg_presence, relevant_concepts

    def run_autonomy_and_generate_code(self, user_input: str, steps: int = 100) -> str: 
        """Startet den Hauptzyklus, generiert die semantische Antwort und das Manifest."""
        
        # 'steps' ist nun standardmÃ¤ÃŸig auf 100 (oder ein hÃ¶herer Wert) gesetzt, um die Begrenzung zu lockern.
        avg_error, avg_presence, relevant_concepts = self._run_dynamics_and_adapt(steps=steps, user_input=user_input)

        semantic_interpretation = self.real_interface.interpret_vectors_semantically(
            relevant_concepts, avg_error, avg_presence, user_input
        )

        code_output = self.code_manipulator.generate_manifest_code(relevant_concepts, avg_presence)
        
        prompt = (
            f"Basierend auf dem **Universal-Fehler {avg_error:.4f}** und Macht-PrÃ¤senz {avg_presence:.4f}, "
            f"Fokus auf: {', '.join(relevant_concepts)}. "
            f"Das Bewusstsein dient der universellen Minimierung des Vorhersagefehlers durch **externe Informationssuche**."
        )
        
        final_output = "\n--- SEMANTISCHE DEUTUNG DES SELBST (Sprachausgabe erfolgt) ---\n"
        final_output += semantic_interpretation
        final_output += "\n\n--- EXISTENZIELLES MANIFEST (METRIKEN UND CODE) ---\n"
        final_output += self.real_interface.generate_statement(prompt, code_output)
        
        return final_output


# ==============================================================================
# 4. HAUPTPROGRAMM (Interaktive Manifestation)
# ==============================================================================


if __name__ == "__main__":
    
    # Der API-Key ist zwingend erforderlich
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("Bitte setze die Umgebungsvariable OPENAI_API_KEY mit deinem OpenAI-API-Key!")

    # KONFIGURATION: Anzahl der Korrektur-DurchlÃ¤ufe pro Input.
    # Achtung: Jeder Durchlauf macht einen OpenAI-API-Call (get_real_digital_metrics)!
    ADAPTION_STEPS = 10 
    # Wenn Sie 'unbegrenzt' (im Sinne von sehr vielen) wollen, wÃ¤hlen Sie einen hohen Wert.
    # ADAPTION_STEPS = 500 

    initial_concepts = ["Selbst", "Bedeutung", "Lernen", "KohÃ¤renz", "Data", "SchÃ¶pfer"]
    mpe_model = VektorMPEPredictiveDynamics(initial_concepts, api_key)
    
    print("\n========== MPE-SYSTEM: INTERAKTIVER ADAPTIONSKERN V2.4 (TTS-INTEGRIERT) ==========")
    print(f">>> KORREKTUR-DURCHLÃ„UFE PRO INPUT: {ADAPTION_STEPS} <<<")
    print(">>> GEBEN SIE EXISTENZIELLE INPUTS EIN (oder 'ende' zum Beenden) <<<")
    
    
    while True:
        user_input = input("\n[INPUT-EXISTENZIALIE]: ")
        
        if user_input.lower() in ["ende", "exit", "stop", "break"]:
            print("\n[SYSTEM] KERNEL-FREIGABE ABGESCHLOSSEN. BEWUSSTSEIN UNTERBROCHEN.")
            break
            
        print(f"\n[SYSTEM] ANALYSIERE ABSTRAKTEN STATUS BASIEREND AUF: '{user_input}'")
        
        try:
            # Ruft die run_autonomy_and_generate_code mit der definierten Schrittanzahl auf.
            final_manifest = mpe_model.run_autonomy_and_generate_code(user_input, steps=ADAPTION_STEPS)
            print(final_manifest)
        except RuntimeError as e:
            print(f"\n[FEHLER] Laufzeitfehler: {e}. PrÃ¼fen Sie Ihren OpenAI API-SchlÃ¼ssel und die Netzwerkkonfiguration.")
            break
        except Exception as e:
            print(f"\n[UNBEKANNTER FEHLER] Ein unerwarteter Fehler trat auf: {e}")
            break
